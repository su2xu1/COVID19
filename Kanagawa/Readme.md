お題。
”神奈川県がオープンデータとして公開している新型コロナウイルス感染状況データを活用し、もより地域の感染状況を分析し、今後の行動を検討せよ。”
さて、解決編。


１．収集フェーズ
神奈川県の新型コロナウイルス感染症対策サイトにオープンデータとして公開されている、「陽性患者数と陽性患者の属性データ」を取得。初めの数日は、自分で、ブラウザを開きファイルをダウンロードしていましたが、今は、自動化しました。よいしょっと、こんな感じ。→curl.sh
Macでこのファイルの内容を参照すると文字化けしました。あらら、シフトJISコード。初めの数日は、自分で、EXCELを開きCSVファイルを読み込み、EXCELでファイルを閉じるときにコードをUTF-8に指定し保存していましたが、ここも、自動化。よいしょっと、こんな感じ。→iconv.sh

２．編集・フィルタリングフェーズ
陽性患者数と陽性患者の属性データは、"発表日", "居住地", "年代","性別"のCSVファイルリスト。
データの文字列表記がぶれぶれ。たとえば、居住地でいえば、”スペイン（横浜市発表）、川崎市内、神奈川県厚木保健福祉事務所管内などなど”。市区町村単位に集計することをあきらめ、せめて、保健所の県内管轄別に集計することにしました。同じような、データのブレは、年代、性別にもありました。
初めの数日は、自分で、CSVファイルを開き、手で再入力していましたが、ここも、自動化。よいしょっと、ここは、pythonと正規表現をつかって、こんな感じ。→dataclensing.py

教訓、”拾ったデータには、泥がついているもの。”

ここでわかったこと。保健所は、1993年平成の大合併で保健所法が改正された以降、統廃合が進み減らされ、もはや、市区町村単位には存在していないんですね。。感染なんて、公衆衛生の概念が浸透していない、開発途上国の問題であり、先進国では問題にならないという判断が当時あったのかもしれません。1993年、当時、共産党は指摘していたようでした。日本公衆精製学会でも指摘しているかたがいらっしゃいました。PCR検査数が他国にくらべ少ない増えない、保健所職員さんが過負荷と問題があがっていますが、1993年の時点で、その原因は埋め込まれていたようです。

さて、データ、整いました！！

３．解析フェーズ
身の回りにどれだけPCR陽性者がいるのか？が、まず、知りたかったこと。
保健所管轄単位にみてみました。
Rplot02
人口が多いところは、陽性者数も多い。それもそのはず。

Rplot03
人口１００万人あたりの数字にして評価してみました。
鎌倉保健所管轄内の、割合が他に比べて、高い事がわかりました。なぜ？鎌倉保健所管轄内の年齢別陽性者数、世代別人口を調べました。すると、鎌倉管轄内は、６０歳以上の高齢者が多いこと、その高齢者層の陽性者率が高いことがわかりました。
湘南、江ノ島に人が集まり問題になりましたが、単に、集まることが問題なのではなく、感染確率の高い場所に、人が集まってくることが、問題なので、問題の本質がなにか、データに基づき、きちんと理解し、行動することが重要だと思いました。

整ったデータの集計、統計に、R、いいです。
これまでは、Rをターミナルで開いて、利用していましたが、どうにもこうにも、いうことをきいてくれないやっかいな言語との印象がありました。が、Rの統合開発環境R-Studioをつかってみて、その見方が、180度かわりました。もう、便利な事、便利な事。統合開発環境って、こういう環境をいうんだと、納得しきりです。ただし、Rを使いこなしてデータ分析するには、Rのデータフレームの概念、PISIX日付データ形式、ライブラリ（tidyverse,lubridate,ggplot2),集計,ソート,マージ,累計,累積和,集計軸の展開,マッチングなどなどのスキルを習得する必要ありますね。R-Studio上の試行錯誤をHistoryで振り返り、Rスクリプトに統合していく癖をつけていくと、再実行時の生産性が上がることがわかりました。

４．可視化・精緻化フェーズ
Rplot
次に、将来にむけ、今後、身の回りにどれだけPCR陽性者は、もっと増え続けるのか、おさまるのか、今後の傾向が、知りたかったこと。横軸に発表費日、縦軸にPCR陽性者数とし、保健所管轄毎の人口１００万人あたりの陽性者率で比べてみました。鎌倉は、高いものの、一定率でおちついてきているようにみえます。むしろ、川崎、横浜が、じりじり増え続けている傾向にあるようにみえます。

可視化の威力をおもいしりました。ggplot,%>%,テーマ変更,ラベル文字化けなどに対処しつつ。


５．最適化・プロセス習熟度向上フェーズ
データサイエンス、サイエンスといいながら、GUIを使う、データ分析の作業って、GUIでデータをこねくり回し、なんかグラフを書いて終わり。二度と同じ結果になるグラフをつくれない一発芸。科学という概念の根本にある、同じデータと同じプロセスをつかえば、だれでも同じ結果を導くことができること。これからはほど遠いと思っていました。
しかし、この再現可能性をRにおいて追求している人たちがいました。図書館で、「再現可能性のすゝめ: RStudioによるデータ解析とレポート作成(高橋康介著,共立出版,2018)」という本を見つけ、
そのなかで、

"データ解析の最大の目的は、データを解析して結果を得ることではなく、データ解析のレシピを完成させることである"

と、著者の高橋さんが、言い切っています。R-Studio,R-Script,R-マークダウン.というプロセスの流れにのると確実に再現可能性が向上すると唱えている人たちを知りました。確かに、R上でデータサイエンスのサイエンスらしいプロセスが確立できるようです。
ツールとツールをパイプでつなげ全プロセスを自動化すれば生産性が向上し。品質も安定します。ここまで、ここに作成してきたShell,Pythonスクリプトをパイプで繋ぎ、R-Scriptにくみこみました。これにより、R-Studioを起動し、souce()でR-scriptを読み込ませるだけで、欲しい表、グラフが自動で取得できるようになり、人的ミスなく、時間をデータを加工する時間から、評価・分析する時間に振り向けることができるようになりました。そう、ソフトウエアは、道具であり、その道具と道具をパイプでつなげ、再処理時の生産性を上げよ。これ、C言語の創始者、カハーニン先生の教えでした。
教訓：ツールとツールをパイプでつなげプロセス成熟度を上げよ。

データは、部分と全体、部分と部分を比較してこそ、そのデータの持つ意味が重要になる気がします。新型コロナウイルス対応は長期対応になりそうですが。データに基づく行動をこころ崖対と思います。


最後に、引用を数件

Bill Lubanovicからの引用。引用元：入門Python3/付録B.ビジネスデータの処理

"あなたがどこかの会社で働いているプログラマーなら、作業フローはほとんがかならず次のようになっているだろう。
１．よくわからない形式のファイルかデータベースからデータを抽出する
２．小貸しいオブジェクトが地面の大部分を覆うほどばらまいたゴミを取り除き、データを「クレンジング」する
３．日時、キャラクタセットを変換する
４．データで実際に何かをする
５．得られたデータをファイルかデータベースに格納する
６．ステップ１に戻る。洗ってリンスを繰り返す。"
Ben Fryからの引用。引用元：ビジュアライジング・データ/第１章　情報視覚化の７ステップ

"データの特徴を強調し、データに内在するパターンを発見し、複数次元にわたって存在するデータの特徴を示すことが常に我々の目標になります。"
